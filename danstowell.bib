
@inproceedings{Lawati:2020,
title={Short-term prediction of photovoltaic power generation using {G}aussian process regression},
  author={Lawati, Yahya Al and Kelly, Jack and Stowell, Dan},
  journal={arXiv preprint arXiv:2010.02275},
  year={2020},
  booktitle={Proceedings of Tackling Climate Change with Machine Learning (workshop at NeurIPS 2020)},
  doi={},
  abstract={Photovoltaic (PV) power is affected by weather conditions, making the power generated from the PV systems uncertain. Solving this problem would help improve the reliability and cost effectiveness of the grid, and could help reduce reliance on fossil fuel plants. The present paper focuses on evaluating predictions of the energy generated by PV systems in the United Kingdom Gaussian process regression (GPR). Gaussian process regression is a Bayesian non-parametric model that can provide predictions along with the uncertainty in the predicted value, which can be very useful in applications with a high degree of uncertainty. The model is evaluated for short-term forecasts of 48 hours against three main factors -- training period, sky area coverage and kernel model selection -- and for very short-term forecasts of four hours against sky area. We also compare very short-term forecasts in terms of cloud coverage within the prediction period and only initial cloud coverage as a predictor. },
}

@inproceedings{Solomes:2020,
  title={Efficient Bird Sound Detection on the {B}ela Embedded System},
  author={Solomes, Alexandru-Marius and Stowell, Dan},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={746--750},
  year={2020},
  organization={IEEE},
  doi={10.1109/ICASSP40776.2020.9053533},
  abstract={Monitoring wildlife is an important aspect of conservation initiatives. Deep learning detectors can help with this, although it is not yet clear whether they can run efficiently on an embedded system in the wild. This paper proposes an automatic detection algorithm for the Bela embedded Linux device for wildlife monitoring. The algorithm achieves good quality recognition, efficiently running on continuously streamed data on a commercially available platform. The program is capable of computing on-board detection using convolutional neural networks (CNNs) with an AUC score of 82.5% on the testing set of an international data challenge. This paper details how the model is exported to work on the Bela Mini in C++, with the spectrogram generation and the implementation of the feed-forward network, and evaluates its performance on the Bird Audio Detection challenge 2018 DCASE data.},
}

@article{Stowell:2020ukpvgeo,
        doi = {10.1038/s41597-020-00739-0},
        url = {https://doi.org/10.1038%2Fs41597-020-00739-0},
        year = 2020,
        month = {nov},
        publisher = {Springer Science and Business Media {LLC}},
        volume = {7},
        number = {1},
        author = {Dan Stowell and Jack Kelly and Damien Tanner and Jamie Taylor and Ethan Jones and James Geddes and Ed Chalstrey},
        title = {A harmonised, high-coverage, open dataset of solar photovoltaic installations in the {UK}},
        journal = {Scientific Data},
  abstract={Solar photovoltaic (PV) is an increasingly significant fraction of electricity generation. Efficient management, and innovations such as short-term forecasting and machine vision, demand high-resolution geographic datasets of PV installations. However, official and public sources have notable deficiencies: spatial imprecision, gaps in coverage and lack of crucial meta data, especially for small-scale solar panel installations. We present the results of a major crowd-sourcing campaign to create open geographic data for over 260,000 solar PV installations across the UK, covering an estimated 86% of the capacity in the country. We focus in particular on capturing small-scale domestic solar PV, which accounts for a significant fraction of generation but was until now very poorly documented. Our dataset suggests nameplate capacities in the UK (as of September 2020) amount to a total of 10.66 GW explicitly mapped, or 13.93 GW when missing capacities are inferred. Our method is applied to the UK but applicable worldwide, and compatible with continual updating to track the rapid growth in PV deployment.},
}

@article{Stowell:2020ecoacou,
author = {Stowell, Dan and Sueur, J{\'{e}}r{\^{o}}me},
title = {Ecoacoustics: acoustic sensing for biodiversity monitoring at scale},
journal = {Remote Sensing in Ecology and Conservation},
volume = {6},
number = {3},
pages = {217--219},
doi = {10.1002/rse2.174},
url = {https://zslpublications.onlinelibrary.wiley.com/doi/abs/10.1002/rse2.174},
year = {2020},
}

@article{Yela:2020online,
  title={Online visibility graphs: Encoding visibility in a binary search tree},
  author={Yela, Delia Fano and Thalmann, Florian and Nicosia, Vincenzo and Stowell, Dan and Sandler, Mark},
  journal={Physical Review Research},
  volume={2},
  number={2},
  pages={023069},
  year={2020},
  publisher={APS},
  doi={10.1103/PhysRevResearch.2.023069},
  abstract={A visibility algorithm maps time series into complex networks following a simple criterion, and the resulting visibility graphs have recently proven to be a powerful tool for time series analysis. However, their direct computation is time-consuming and rigid, motivating the development of more efficient algorithms. Here we introduce a description of a method to compute visibility graphs online, which is highly efficient, compatible with batchwise progressive updates, and capable of assimilating new data without having to recompute the graph from scratch. We use a binary search tree to encode and store visibility relations, which can be decoded at a later stage into a visibility graph. The proposed encoder/decoder approach offers an online computation solution at no additional computational cost and makes it possible to use visibility graphs for large-scale time series analysis and for applications where online data assimilation is required.},
}

@inproceedings{Matt:2019,
        title={Estimating & Mitigating the Impact of Acoustic Environments on Machine-to-Machine Signalling},
        year={2019},
        booktitle={Proc EUSIPCO 2019},
        author={Matt, A. and Stowell, D.},
  doi={10.23919/EUSIPCO.2019.8902634},
  abstract={The advance of technology for transmitting Data-over-Sound in various IoT and telecommunication applications has led to the concept of machine-to-machine over-the-air acoustic signalling. Reverberation can have a detrimental effect on such machine-to-machine signals while decoding. Various methods have been studied to combat the effects of reverberation in speech and audio signals, but it is not clear how well they generalise to other sound types. We look at extending these models to facilitate machine-to-machine acoustic signalling. This research investigates dereverberation techniques to shortlist a single-channel reverberation suppression method through a pilot test. In order to apply the chosen dereverberation method a novel method of estimating acoustic parameters governing reverberation is proposed. The performance of the final algorithm is evaluated on quality metrics as well as the performance of a real machine-to-machine decoder. We demonstrate a dramatic reduction in error rate for both audible and ultrasonic signals.},
}

@article{Morfi:2019nips4bplus,
  title={NIPS4Bplus: a richly annotated birdsong audio dataset},
  author={Morfi, Veronica and Bas, Yves and Pamu{\l}a, Hanna and Glotin, Herv{\'e} and Stowell, Dan},
  journal={PeerJ Computer Science},
  volume={5},
  pages={e223},
  year={2019},
  publisher={PeerJ Inc.},
  doi={10.7717/peerj-cs.223},
  abstract={Recent advances in birdsong detection and classification have approached a limit due to the lack of fully annotated recordings. In this paper, we present NIPS4Bplus, the first richly annotated birdsong audio dataset, that is comprised of recordings containing bird vocalisations along with their active species tags plus the temporal annotations acquired for them. Statistical information about the recordings, their species specific tags and their temporal annotations are presented along with example uses. NIPS4Bplus could be used in various ecoacoustic tasks, such as training models for bird population monitoring, species classification, birdsong vocalisation detection and classification.},
}

@inproceedings{Wilkinson:2019icml,
        author={Wilkinson, W.J. and {Riis Andersen}, M. and Reiss, J.D. and Stowell, D. and Solin, A.},
        title={End-to-End Probabilistic Inference for Nonstationary Audio Analysis},
        booktitle={Proceedings of the Internaional Conference on Machine Learning (ICML)},
        year={2019},
  pages={6776--6785},
        abstract={A typical audio signal processing pipeline includes multiple disjoint analysis stages, including calculation of a time-frequency representation followed by spectrogram-based feature analysis. We show how time-frequency analysis and nonnegative matrix factorisation can be jointly formulated as a spectral mixture Gaussian process model with nonstationary priors over the amplitude variance parameters. Further, we formulate this nonlinear model's state space representation, making it amenable to infinite-horizon Gaussian process regression with approximate inference via expectation propagation, which scales linearly in the number of time steps and quadratically in the state dimensionality. By doing so, we are able to process audio signals with hundreds of thousands of data points. We demonstrate, on various tasks with empirical data, how this inference scheme outperforms more standard techniques that rely on extended Kalman filtering.},
}

@article{Stowell:2019bgaugj,
        doi = {10.1098/rsif.2018.0940},
        url = {https://doi.org/10.1098/rsif.2018.0940},
        year = 2019,
        month = {apr},
        publisher = {The Royal Society},
        volume = {16},
        number = {153},
        author = {Dan Stowell and Tereza Petruskov{\'{a}} and Martin {\v{S}}{\'{a}}lek and Pavel Linhart},
        title = {Automatic acoustic identification of individuals in multiple species: improving identification across recording conditions},
        journal = {Journal of The Royal Society Interface},
  abstract={Many animals emit vocal sounds which, independently from the sounds’ function, contain some individually distinctive signature. Thus the automatic recognition of individuals by sound is a potentially powerful tool for zoology and ecology research and practical monitoring. Here, we present a general automatic identification method that can work across multiple animal species with various levels of complexity in their communication systems. We further introduce new analysis techniques based on dataset manipulations that can evaluate the robustness and generality of a classifier. By using these techniques, we confirmed the presence of experimental confounds in situations resembling those from past studies. We introduce data manipulations that can reduce the impact of these confounds, compatible with any classifier. We suggest that assessment of confounds should become a standard part of future studies to ensure they do not report over-optimistic results. We provide annotated recordings used for analyses along with this study and we call for dataset sharing to be a common practice to enhance the development of methods and comparisons of results.},
}

@inproceedings{Wilkinson:2019unifying,
  title={Unifying probabilistic models for time-frequency analysis},
  author={Wilkinson, William J and Andersen, Michael Riis and Reiss, Joshua D and Stowell, Dan and Solin, Arno},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={3352--3356},
  year={2019},
  organization={IEEE},
  doi={10.1109/ICASSP.2019.8682306},
  abstract={In audio signal processing, probabilistic time-frequency models have many benefits over their non-probabilistic counterparts. They adapt to the incoming signal, quantify uncertainty, and measure correlation between the signal's amplitude and phase information, making time domain resynthesis straightforward. However, these models are still not widely used since they come at a high computational cost, and because they are formulated in such a way that it can be difficult to interpret all the modelling assumptions. By showing their equivalence to Spectral Mixture Gaussian processes, we illuminate the underlying model assumptions and provide a general framework for constructing more complex models that better approximate real-world signals. Our interpretation makes it intuitive to inspect, compare, and alter the models since all prior knowledge is encoded in the Gaussian process kernel functions. We utilise a state space representation to perform efficient inference via Kalman smoothing, and we demonstrate how our interpretation allows for efficient parameter learning in the frequency domain.},
}



@inproceedings{Alvarado:2019sparse,
  title={Sparse gaussian process audio source separation using spectrum priors in the time-domain},
  author={Alvarado, Pablo A and Alvarez, Mauricio A and Stowell, Dan},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={995--999},
  year={2019},
  organization={IEEE},
  doi={10.1109/ICASSP.2019.8683287},
  abstract={Gaussian process (GP) audio source separation is a time- domain approach that circumvents the inherent phase approx- imation issue of spectrogram based methods. Furthermore, through its kernel, GPs elegantly incorporate prior knowl- edge about the sources into the separation model. Despite these compelling advantages, the computational complexity of GP inference scales cubically with the number of audio samples. As a result, source separation GP models have been restricted to the analysis of short audio frames. We intro- duce an efficient application of GPs to time-domain audio source separation, without compromising performance. For this purpose, we used GP regression, together with spectral mixture kernels, and variational sparse GPs. We compared our method with LD-PSDTF (positive semi-definite tensor factorization), KL-NMF (Kullback-Leibler non-negative ma- trix factorization), and IS-NMF (Itakura-Saito NMF). Results show that the proposed method outperforms these techniques.},
}


@inproceedings{Yela:2019spectral,
  title={Spectral Visibility Graphs: Application to Similarity of Harmonic Signals},
  author={Yela, Delia Fano and Stowell, Dan and Sandler, Mark},
  booktitle={2019 27th European Signal Processing Conference (EUSIPCO)},
  pages={1--5},
  year={2019},
  organization={IEEE},
  doi={10.23919/EUSIPCO.2019.8903055},
  abstract={Graph theory is emerging as a new source of tools for time series analysis. One promising method is to transform a signal into its visibility graph, a representation which captures many interesting aspects of the signal. Here we introduce the visibility graph for audio spectra and propose a novel representation for audio analysis: the spectral visibility graph degree. Such representation inherently captures the harmonic content of the signal whilst being resilient to broadband noise. We present experiments demonstrating its utility to measure robust similarity between harmonic signals in real and synthesised audio data. The source code is available online.},
}














@article{Morfi:2018j,
  title={Deep Learning for Audio Event Detection and Tagging on Low-Resource Datasets},
  author={Morfi, V. and Stowell, D.},
  journal={Applied Sciences},
  year={2018},
  volume={8},
  issue={8},
  pages={1397},
  url={https://doi.org/10.3390/app8081397 },
  doi={10.3390/app8081397 },
  abstract={In training a deep learning system to perform audio transcription, two practical problems may arise. Firstly, most datasets are weakly labelled, having only a list of events present in each recording without any temporal information for training. Secondly, deep neural networks need a very large amount of labelled training data to achieve good quality performance, yet in practice it is difficult to collect enough samples for most classes of interest. In this paper, we propose factorising the final task of audio transcription into multiple intermediate tasks in order to improve the training performance when dealing with this kind of low-resource datasets. We evaluate three data-efficient approaches of training a stacked convolutional and recurrent neural network for the intermediate tasks. Our results show that different methods of training have different advantages and disadvantages.},
}

@article{Stowell:2018badchj,
   author = {Stowell, D. and Stylianou, Y. and Wood, M. and Pamu{\l}a, H. and Glotin, H.},
    title = {Automatic acoustic detection of birds through deep learning: the first Bird Audio Detection challenge},
  journal = {Methods in Ecology and Evolution},
   eprint = {1807.05812},
     year = 2019,
    month = Oct,
  doi={10.1111/2041-210X.13103},
  pages = {368--380
},
  abstract={Assessing the presence and abundance of birds is important for monitoring specific species as well as overall ecosystem health. Many birds are most readily detected by their sounds, and thus passive acoustic monitoring is highly appropriate. Yet acoustic monitoring is often held back by practical limitations such as the need for manual configuration, reliance on example sound libraries, low accuracy, low robustness, and limited ability to generalise to novel acoustic conditions. Here we report outcomes from a collaborative data challenge showing that with modern machine learning including deep learning, general-purpose acoustic bird detection can achieve very high retrieval rates in remote monitoring data --- with no manual recalibration, and no pre-training of the detector for the target species or the acoustic conditions in the target environment. Multiple methods were able to attain performance of around 88% AUC (area under the ROC curve), much higher performance than previous general-purpose methods. We present new acoustic monitoring datasets, summarise the machine learning techniques proposed by challenge teams, conduct detailed performance evaluation, and discuss how such approaches to detection can be integrated into remote monitoring projects. },
}

@incollection{Morfi:2018multitask,
   author = {{Morfi}, V. and {Stowell}, D.},
    title = {Deep Learning for Audio Transcription on Low-Resource Datasets},
booktitle = {Proceedings of the 2018 DCASE workshop,},
      url = {https://arxiv.org/abs/1807.06972},
   eprint = {1807.03697},
     year = 2018,
  abstract={In training a deep learning system to perform audio transcription, two practical problems may arise. Firstly, most datasets are weakly labelled, having only a list of events present in each recording without any temporal information for training. Secondly, deep neural networks need a very large amount of labelled training data to achieve good quality performance, yet in practice it is difficult to collect enough samples for most classes of interest. In this paper, we propose factorising the final task of audio transcription into multiple intermediate tasks in order to improve the training performance when dealing with this kind of low-resource datasets. We evaluate three data-efficient approaches of training a stacked convolutional and recurrent neural network for the intermediate tasks. Our results show that different methods of training have different advantages and disadvantages. },
  annote={
},
}



@inproceedings{Yela:2018lva,
  title={Does k Matter? k-NN Hubness Analysis for Kernel Additive Modelling Vocal Separation},
  author={Yela, Delia Fano and Stowell, Dan and Sandler, Mark},
  booktitle={International Conference on Latent Variable Analysis and Signal Separation},
  pages={280--289},
  year={2018},
  organization={Springer},
  doi={10.1007/978-3-319-93764-9_27},
  url={https://arxiv.org/abs/1804.02325},
  abstract={Kernel Additive Modelling (KAM) is a framework for source separation aiming to explicitly model inherent properties of sound sources to help with their identification and separation. KAM separates a given source by applying robust statistics on the selection of time-frequency bins obtained through a source-specific kernel, typically the k-NN function. Even though the parameter k appears to be key for a successful separation, little discussion on its influence or optimisation can be found in the literature. Here we propose a novel method, based on graph theory statistics, to automatically optimise k in a vocal separation task. We introduce the k-NN hubness as an indicator to find a tailored k at a low computational cost. Subsequently, we evaluate our method in comparison to the common approach to choose k. We further discuss the influence and importance of this parameter with illuminating results.},
}

@inproceedings{Wilkinson:2018lva,
  title={A Generative Model for Natural Sounds Based on Latent Force Modelling},
  author={Wilkinson, William J and Reiss, Joshua D and Stowell, Dan},
  booktitle={International Conference on Latent Variable Analysis and Signal Separation (LVA/ICA)},
  pages={259--269},
  year={2018},
  organization={Springer},
  doi={10.1007/978-3-319-93764-9_25},
  url={https://arxiv.org/abs/1802.00680},
  abstract={Generative models based on subband amplitude envelopes of natural sounds have resulted in convincing synthesis, showing subband amplitude modulation to be a crucial component of auditory perception. Probabilistic latent variable analysis can be particularly insightful, but existing approaches don’t incorporate prior knowledge about the physical behaviour of amplitude envelopes, such as exponential decay or feedback. We use latent force modelling, a probabilistic learning paradigm that encodes physical knowledge into Gaussian process regression, to model correlation across spectral subband envelopes. We augment the standard latent force model approach by explicitly modelling dependencies across multiple time steps. Incorporating this prior knowledge strengthens the interpretation of the latent functions as the source that generated the signal. We examine this interpretation via an experiment showing that sounds generated by sampling from our probabilistic model are perceived to be more realistic than those generated by comparative models based on nonnegative matrix factorisation, even in cases where our model is outperformed from a reconstruction error perspective.},
}

@inproceedings{Pamula:2017,
        title={Adaptation of deep learning methods to nocturnal bird audio monitoring},
        booktitle={LXIV Open Seminar on Acoustics (OSA) 2017},
        month={Sep},
        address={Piekary {\.S}l{\c a}skie, Poland},
        year={2017},
        author={Hanna Pamu{\l}a and Maciej K{\l}aczynski and Magdalena Remisiewicz and Wies{\l}aw Wszo{\l}ek and Dan Stowell},
        abstract={Trends in the size of bird populations are good indicators of the general state of the environment.
Different bird monitoring methods have been developed over the years, but surveys based on
observations and bird-ringing programmes are the most common techniques. However, counting
nocturnal migrant birds remains a particularly difficult task so other techniques must be applied. One
possible method is automatic acoustic monitoring, which can supplement standard bird-monitoring
schemes.
We investigated the automatic detection of migrant birds’ night flight calls. Long-term recordings
were collected during birds’ autumn migration along the Baltic Sea coast. A deep learning method,
convolutional neural network working on spectrograms, was adapted to detect the migrants’ calls.
The first results are promising (AUC=96.6%), showing the potential of acoustic methods to
supplement standard bird-monitoring techniques and to suggest a directions for research.},
        url={},
}


@incollection{Stowell:2017chapter,
  title={Computational Bioacoustic Scene Analysis},
  author={Stowell, D.},
  booktitle={Computational Analysis of Sound Scenes and Events},
  pages={303--333},
  year={2018},
  chapter={11},
  publisher={Springer},
  doi={10.1007/978-3-319-63450-0_11},
  abstract={The analysis of natural and animal sound makes a demonstrable contribution to important challenges in conservation, animal behaviour, and evolution. And now bioacoustics has entered its big data era. Thus automation is important, as is scalability in many cases to very large amounts of audio data and to real-time processing. This chapter will focus on the data science and the computational methods that can enable this. Computational bioacoustics has some commonalities with wider audio scene analysis, as well as with speech processing and other disciplines. However, the tasks required and the specific characteristics of bioacoustic data require new and adapted techniques. This chapter will survey the tasks and the methods of computational bioacoustics, and will place particular emphasis on existing work and future prospects which address scalable analysis. We will mostly focus on airborne sound; there has also been much work on freshwater and marine bioacoustics, and a small amount on solid-borne sounds.},
  annote={
},
}

@incollection{Benetos:2017chapter,
  title={Approaches to Complex Sound Scene Analysis},
  author={Benetos, E. and Stowell, D. and Plumbley, M. D},
  booktitle={Computational Analysis of Sound Scenes and Events},
  pages={215--242},
  year={2018},
  chapter={8},
  publisher={Springer},
  doi={10.1007/978-3-319-63450-0_8},
  abstract={This chapter presents state-of-the-art research and open topics for analyzing complex sound scenes in a single microphone case. First, the concept of sound scene recognition is presented, from the perspective of different paradigms (classification, tagging, clustering, segmentation) and methods used. The core section is on sound event detection and classification, presenting various paradigms and practical considerations along with methods for monophonic and polyphonic sound event detection. The chapter will then focus on the concepts of context and “language modeling” for sound scenes, also covering the concept of relationships between sound events. Work on sound scene recognition based on event detection is also presented. Finally the chapter will summarize the topic and will provide directions for future research.},
  annote={
},
}


@techreport{Stowell:2017algorithmic,
        url={http://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/science-and-technology-committee/algorithms-in-decisionmaking/written/69040.html},
        title={Algorithms in decision-making: Written evidence submission},
        author={Stowell, D. and Benetos, E. and Sturm, B. and Tokarchuk, L.},
        abstract={We are a group of academics based in the Centre for Intelligent Sensing at Queen Mary University of London (QMUL). Our work has a strong focus on artificial intelligence, machine learning and other algorithmic approaches which make inferences based on sensor data: primarily sound recordings as well as mobile phone sensor data (see Appendix for definitions of technical terms such as “machine learning”). Our work concerns the engineering of such “intelligent sensing” systems. We also work with commercial partners on data driven applications. In this submission we make recommendations on accountability and bias in decisions made automatically by algorithms such as those driven by machine learning.},
        year={2017},
        month={April},
        institution={Centre for Intelligent Sensing, Queen Mary University of London},
        note={UK Parliament House of Commons Science and Technology Committee, Algorithms in decision-making inquiry},
        number={ALG0036},
}


@incollection{wilkinson:2017dafx,
  title={Latent force models for sound: Learning modal synthesis parameters and excitation functions from audio recordings},
  author={Wilkinson, W. J and Reiss, J. D and Stowell, D.},
  year={2017},
  booktitle={DAFX 2017},
  doi={},
  abstract={Latent force models are a Bayesian learning technique that combine physical knowledge with dimensionality reduction — sets of coupled differential equations are modelled via shared dependence on a low-dimensional latent space. Analogously, modal sound synthesis is a technique that links physical knowledge about the vibration of objects to acoustic phenomena that can be observed in data. We apply latent force modelling to sinusoidal models of audio recordings, simultaneously inferring modal synthesis parameters (stiffness and damping) and the excitation or contact force required to reproduce the behaviour of the observed vibrational modes. Exposing this latent excitation function to the user constitutes a controllable synthesis method that runs in real time and enables sound morphing through interpolation of learnt parameters.},
  annote={
},
}


@inproceedings{Morfi:2017deductive,
  title={Deductive Refinement of Species Labelling in Weakly Labelled Birdsong Recordings},
  author={Morfi, V. and Stowell, D.},
  booktitle={Proc ICASSP 2017},
  doi={10.1109/ICASSP.2017.7952237},
  pages={656--660},
  year={2017},
  abstract={Many approaches have been used in bird species classification from their sound in order to provide labels for the whole of a recording. However, a more precise classification of each bird vocalization would be of great importance to the use and management of sound archives and bird monitoring. In this work, we introduce a technique that using a two step process can first automatically detect all bird vocalizations and then, with the use of ‘weakly’ labelled recordings, classify them. Evaluations of our proposed method show that it achieves a correct classification of 75.4% when used in a synthetic dataset.},
}

@incollection{Alvarado:2016,
  title={Gaussian Processes for Music Audio Modelling and Content Analysis},
  author={Alvarado, P. A. and Stowell, D.},
  booktitle={Proceedings of MLSP 2016},
  year={2016},
  abstract={Real music signals are highly variable, yet they have strong statistical structure. Prior information about the underlying physical mechanisms by which sounds are generated and rules by which complex sound structure is constructed (notes, chords, a complete musical score), can be naturally unified using Bayesian modelling techniques. Typically algorithms for Automatic Music Transcription independently carry out individual tasks such as multiple-F0 detection and beat tracking. The challenge remains to perform joint estimation of all parameters. We present a Bayesian approach for modelling music audio, and content analysis. The proposed methodology based on Gaussian processes seeks joint estimation of multiple music concepts by incorporating into the kernel prior information about non-stationary behaviour, dynamics, and rich spectral content present in the modelled music signal. We illustrate the benefits of this approach via two tasks: pitch estimation, and inferring missing segments in a polyphonic audio recording. },
}

@incollection{Stowell:2016d,
  title={Individual identity in songbirds: signal representations and metric learning for locating the information in complex corvid calls},
  author={Stowell, D. and Morfi, Veronica and Gill, Lisa F},
  booktitle={Proceedings of InterSpeech 2016},
  year={2016},
        month = {sep},
        keywords={individID},
        publisher = {International Speech Communication Association},
  doi = {10.21437/interspeech.2016-465},
  abstract={Bird calls range from simple tones to rich dynamic multi-harmonic structures. The more complex calls are very poorly understood at present, such as those of the scientifically important corvid family (jackdaws, crows, ravens, etc.). Individual birds can recognise familiar individuals from calls, but where in the signal is this identity encoded? We studied the question by applying a combination of feature representations to a dataset of jackdaw calls, including linear predictive coding (LPC) and adaptive discrete Fourier transform (aDFT). We demonstrate through a classification paradigm that we can strongly outperform a standard spectrogram representation for identifying individuals, and we apply metric learning to determine which time-frequency regions contribute most strongly to robust individual identification. Computational methods can help to direct our search for understanding of these complex biological signals.},
}

@incollection{Stowell:2016c,
  title={Bird detection in audio: a survey and a challenge},
  author={Stowell, D. and Wood, Mike and Stylianou, Yannis and Glotin, Herv{\'e}},
  booktitle={Proceedings of MLSP 2016},
  year={2016},
  doi={10.1109/MLSP.2016.7738875},
  abstract={Many biological monitoring projects rely on acoustic detection of birds. Despite increasingly large datasets, this detection is often manual or semi-automatic, requiring manual tuning/postprocessing. We review the state of the art in automatic bird sound detection, and identify a widespread need for tuning-free and species-agnostic approaches. We introduce new datasets and an IEEE research challenge to address this need, to make possible the development of fully automatic algorithms for bird sound detection. },
}

@article{Stowell:2017,
        title={On-bird Sound Recordings: Automatic Acoustic Recognition of Activities and Contexts},
        author={Stowell, D. and Benetos, E. and Gill, L. F.},
        journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
        year={2017},
        month={June},
        doi={10.1109/TASLP.2017.2690565},
        volume={25},
        number={6},
        pages={1193--1206},
	abstract={We introduce a novel approach to studying animal behavior and the context in which it occurs, through the use of microphone backpacks carried on the backs of individual free-flying birds. These sensors are increasingly used by animal behavior researchers to study individual vocalizations of freely behaving animals, even in the field. However, such devices may record more than an animal's vocal behavior, and have the potential to be used for investigating specific activities (movement) and context (background) within which vocalizations occur. To facilitate this approach, we investigate the automatic annotation of such recordings through two different sound scene analysis paradigms: A scene-classification method using feature learning, and an event-detection method using probabilistic latent component analysis. We analyze recordings made with Eurasian jackdaws (Corvus monedula) in both captive and field settings. Results are comparable with the state of the art in sound scene analysis; we find that the current recognition quality level enables scalable automatic annotation of audio logger data, given partial annotation, but also find that individual differences between animals and/or their backpacks limit the generalization from one individual to another. we consider the interrelation of “scenes” and “events” in this particular task, and issues of temporal resolution.}
}

@article{Stowell:2016,
        title={Detailed temporal structure of communication networks in groups of songbirds},
        author={Stowell, D. and Gill, L. F. and Clayton, D.},
        journal={Journal of the Royal Society Interface},
        volume={13},
        number={119},
        year={2016},
        doi={10.1098/rsif.2016.0296},
        abstract={Animals in groups often exchange calls, in patterns whose temporal structure may be influenced by contextual factors such as physical location and the social network structure of the group. We introduce a model-based analysis for temporal patterns of animal call timing, originally developed for networks of firing neurons. This has advantages over cross-correlation analysis in that it can correctly handle common-cause confounds and provides a generative model of call patterns with explicit parameters for the influences between individuals. It also has advantages over standard Markovian analysis in that it incorporates detailed temporal interactions which affect timing as well as sequencing of calls. Further, a fitted model can be used to generate novel synthetic call sequences. We apply the method to calls recorded from groups of domesticated zebra finch (Taeniopygia guttata) individuals. We find that the communication network in these groups has stable structure that persists from one day to the next, and that “kernels” reflecting the temporal range of influence have a characteristic structure for a calling individual’s effect on itself, its partner, and on others in the group. We further find characteristic patterns of influences by call type as well as by individual.},
}

@inproceedings{Stowell:2015b,
	author={Stowell, D. and Clayton, D.},
	title={Acoustic event detection for multiple overlapping similar sources },
	pages={},
	booktitle={Proceedings of the {IEEE} Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
	year={2015},
	url={http://arxiv.org/abs/1503.07150},
	abstract={Many current paradigms for acoustic event detection (AED) are not adapted to the organic variability of natural sounds, and/or they assume a limit on the number of simultaneous sources: often only one source, or one source of each type, may be active. These aspects are highly undesirable for applications such as bird population monitoring. We introduce a simple method modelling the onsets, durations and offsets of acoustic events to avoid intrinsic limits on polyphony or on inter-event temporal patterns. We evaluate the method in a case study with over 3000 zebra finch calls. In comparison against a HMM-based method we find it more accurate at recovering acoustic events, and more robust for estimating calling rates. },
}

@article{Stowell:2015,
	title={Detection and classification of acoustic scenes and events},
	journal={{IEEE} Transactions on Multimedia},
	author={Stowell, D. and Giannoulis, D. and Benetos, E. and Lagrange, M. and Plumbley, M. D.},
	year={2015},
	doi={10.1109/TMM.2015.2428998},
	keywords={dcase},
	abstract={For intelligent systems to make best use of the audio modality, it is important that they can recognise not just
speech and music, which have been researched as specific tasks, but also general sounds in everyday environments.
To stimulate research in this field we conducted a public research challenge: the IEEE Audio and Acoustic Signal
Processing Technical Committee challenge on Detection and Classification of Acoustic Scenes and Events (DCASE).
In this paper we report on the state of the art in automatically classifying audio scenes, and automatically detecting
and classifying audio events. We survey prior work as well as the state of the art represented by the submissions to
the challenge from various research groups. We also provide detail on the organisation of the challenge, so that our
experience as challenge hosts may be useful to those organising challenges in similar domains. We created new audio
datasets and baseline systems for the challenge: these, as well as some submitted systems, are publicly available
under open licenses, to serve as benchmark for further research in general-purpose machine listening.},
}

@article{Stowell:2014c,
	author={Stowell, D.},
	title={Renewal processes and semi-{M}arkov processes in animal vocalisations: A response to {K}ershenbaum et al.},
	year={2014},
	url={http://rspb.royalsocietypublishing.org/content/281/1792/20141370.e-letters},
	journal={Proceedings of the Royal Society B},
	volume={281},
	number={eLetters},
}

@article{Stowell:2014b,
	Author = {Stowell, D. and Plumbley, M. D.},
	Title = {Automatic large-scale classification of bird sounds is strongly improved by unsupervised feature learning},
	journal = {PeerJ},
	volume={2},
	pages = {e488},
	doi = {10.7717/peerj.488},
	Year = {2014},
	Eprint = {arXiv:1405.6524},
	abstract = {Automatic species classification of birds from their sound is a computational tool of increasing importance in ecology, conservation monitoring and vocal communication studies. To make classification useful in practice, it is crucial to improve its accuracy while ensuring that it can run at big data scales. Many approaches use acoustic measures based on spectrogram-type data, such as the Mel-frequency cepstral coefficient (MFCC) features which represent a manually-designed summary of spectral information. However, recent work in machine learning has demonstrated that features learnt automatically from data can often outperform manually-designed feature transforms. Feature learning can be performed at large scale and “unsupervised”, meaning it requires no manual data labelling, yet it can improve performance on “supervised” tasks such as classification. In this work we introduce a technique for feature learning from large volumes of bird sound recordings, inspired by techniques that have proven useful in other domains. We experimentally compare twelve different feature representations derived from the Mel spectrum (of which six use this technique), using four large and diverse databases of bird vocalisations, classified using a random forest classifier. We demonstrate that in our classification tasks, MFCCs can often lead to worse performance than the raw Mel spectral data from which they are derived. Conversely, we demonstrate that unsupervised feature learning provides a substantial boost over MFCCs and Mel spectra without adding computational complexity after the model has been trained. The boost is particularly notable for single-label classification tasks at large scale. The spectro-temporal activations learned through our procedure resemble spectro-temporal receptive fields calculated from avian primary auditory forebrain. However, for one of our datasets, which contains substantial audio data but few annotations, increased performance is not discernible. We study the interaction between dataset characteristics and choice of feature representation through further empirical analysis.},
}

@article{Stowell:2014,
	title={Large-scale analysis of frequency modulation in birdsong databases},
	journal={Methods in Ecology and Evolution},
  author={Stowell, D. and Plumbley, M. D.},
  year={2014},
  doi={10.1111/2041-210X.12223},
  url={http://arxiv.org/abs/1311.4764},
  volume={},
  abstract={Birdsong often contains large amounts of rapid frequency modulation (FM). It is believed that the use or otherwise of FM is adaptive to the acoustic environment and also that there are specific social uses of FM such as trills in aggressive territorial encounters. Yet temporal fine detail of FM is often absent or obscured in standard audio signal analysis methods such as Fourier analysis or linear prediction. Hence, it is important to consider high-resolution signal processing techniques for analysis of FM in bird vocalizations. If such methods can be applied at big data scales, this offers a further advantage as large data sets become available.
    We introduce methods from the signal processing literature which can go beyond spectrogram representations to analyse the fine modulations present in a signal at very short time-scales. Focusing primarily on the genus Phylloscopus, we investigate which of a set of four analysis methods most strongly captures the species signal encoded in birdsong. We evaluate this through a feature selection technique and an automatic classification experiment. In order to find tools useful in practical analysis of large data bases, we also study the computational time taken by the methods, and their robustness to additive noise and MP3 compression.
    We find three methods which can robustly represent species-correlated FM attributes and can be applied to large data sets, and that the simplest method tested also appears to perform the best. We find that features representing the extremes of FM encode species identity supplementary to that captured in frequency features, whereas bandwidth features do not encode additional information.
    FM analysis can extract information useful for bioacoustic studies, in addition to measures more commonly used to characterize vocalizations. Further, it can be applied efficiently across very large data sets and archives.},
  keywords={fmbird},
}

@inproceedings{Stowell:2013n,
  title={Feature design for multilabel bird song classification in noise (NIPS4B challenge)},
  author={Stowell, D. and Plumbley, M. D.},
  year={2013},
  booktitle={Proceedings of NIPS4b: Neural Information Processing Scaled for Bioacoustics, from Neurons to Big Data},
  abstract={},
}

@article{Barchiesi:2015,
  title={Acoustic Scene Classification: Classifying environments from the sounds they produce},
  author={Barchiesi, D. and Giannoulis, D. and Stowell, D. and Plumbley, M. D.},
  journal={IEEE Signal Processing Magazine},
  year={2015},
  volume={32},
  number={3},
  pages={16--34},
  doi={10.1109/MSP.2014.2326181},
  abstract={In this article we present an account of the state-of-the-art in acoustic scene classification ({ASC}), the task of classifying environments from the sounds they produce. Starting from a historical review of previous research in this area, we define a general framework for {ASC} and present different implementations of its components. We then describe a range of different algorithms submitted for a data challenge that was held to provide a general and fair benchmark for {ASC} techniques. The dataset recorded for this purpose is presented, along with the performance metrics that are used to evaluate the algorithms and statistical significance tests to compare the submitted methods. We use a baseline method that employes {MFCCs}, {GMMs} and a maximum likelihood criterion as a benchmark, and only find sufficient evidence to conclude that three algorithms significantly outperform it. We also evaluate the human classification accuracy in performing a similar classification task. The best performing algorithm achieves a mean accuracy that matches the median accuracy obtained by humans, and common pairs of classes are misclassified by both computers and humans. However, all acoustic scenes are correctly classified by at least some individuals, while there are scenes that are misclassified by all algorithms.},
}

@inproceedings{Stowell:2014f,
  title={An open dataset for research on audio field recording archives: freefield1010},
  booktitle={Proceedings of the Audio Engineering Society 53rd Conference on Semantic Audio (AES53)},
  author={Stowell, D. and Plumbley, M. D.},
  year={2014},
  publisher={Audio Engineering Society},
  doi={},
  abstract={We introduce a free and open dataset of 7690 audio clips sampled from the field-recording tag in the Freesound audio archive. The dataset is designed for use in research related to data mining in audio archives of field recordings / soundscapes. Audio is standardised, and audio and metadata are Creative Commons licensed. We describe the data preparation process, characterise the dataset descriptively, and illustrate its use through an auto-tagging experiment. },
  annote={This won the SoundSoftware prize for ``Reproducibility-enabling work''. Review report:

``Panel's comments: This submission received very high scores for sustainability planning and potential to enable high-quality research. The authors have taken a pragmatic approach to availability and portability of the dataset in terms of data formats and packaging, have used redundant hosting, and have documented the dataset well. External reviewers described this submission as lowering the threshold for sound classification and auto-tagging research.''

	http://soundsoftware.ac.uk/rr-prize-aes53-winners
},
}

@incollection{Stowell:2014o,
	title={Case study: How {OpenStreetMap} used humans and machines to map affected areas after {T}yphoon {H}aiyan},
	author={Stowell, D.},
	booktitle={The Verification Handbook},
	editor={Silverman, C.},
	url={http://verificationhandbook.com/book/chapter7.1.php},
	year={2014},
	month={Jan},
	publisher={European Journalism Centre},
}

@inproceedings{Giannoulis:2013b,
	author={Giannoulis, D. and Benetos, E. and Stowell, D. and Rossignol, M. and Lagrange, M. and Plumbley, M. D.},
	title={Detection and classification of acoustic scenes and events: an {IEEE AASP} challenge},
	booktitle={Proceedings of the Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
	year={2013},
  abstract={This paper describes a newly-launched public evaluation challenge on acoustic scene classification and detection of sound events within a scene. Systems dealing with such tasks are far from exhibiting human-like performance and robustness. Undermining factors are numerous: the extreme variability of sources of interest possibly interfering, the presence of complex background noise as well as room effects like reverberation. The proposed challenge is an attempt to help the research community move forward in defining and studying the aforementioned tasks. Apart from the challenge description, this paper provides an overview of systems submitted to the challenge as well as a detailed evaluation of the results achieved by those systems.},
}

@inproceedings{Giannoulis:2013,
  title={A Database and Challenge for Acoustic Scene Classification and Event Detection},
  author={Giannoulis, D. and Stowell, D. and Benetos, E. and Rossignol, M. and Lagrange, M. and Plumbley, M. D.},
  booktitle={Proceedings of the European Signal Processing Conference (EUSIPCO 2013)},
  year={2013},
  pages={1--5},
  abstract={An increasing number of researchers work in computational auditory scene analysis (CASA). However, a set of tasks, each with a well-defined evaluation framework and commonly used datasets do not yet exist. Thus, it is difficult for results and algorithms to be compared fairly, which hinders research on the field. In this paper we will introduce a newly-launched public evaluation challenge dealing with two closely related tasks of the field: acoustic scene classification and event detection. We give an overview of the tasks involved; describe the processes of creating the dataset; and define the evaluation metrics. Finally, illustrations on results for both tasks using baseline methods applied on this dataset are presented, accompanied by open-source code.
},
}

@inproceedings{Stowell:2013c,
   author = {Stowell, D. and Plumbley, M.~D.},
    title = {Acoustic detection of multiple birds in environmental audio by Matching Pursuit},
     archivePrefix = "arXiv",
     year = {2013},
     booktitle = {Proc ICML 2013 Workshop on Machine Learning for Bioacoustics},
     abstract= {We describe a submission to the ICML 2013 Bird Challenge, in which we explore the use of sparse representations as an advance on the standard technique of cross-correlation template matching in time-frequency representations. The Matching Pursuit algorithm is used to represent the signal as a sparse set of activations of templates derived from the challenge training audio.
}
}

@inproceedings{Stowell:2013d,
   author = {Stowell, D. and Mu{\v s}evi{\v c}, S. and Bonada, J. and Plumbley, M.~D.},
    title = {Improved multiple birdsong tracking with distribution derivative method and {M}arkov renewal process clustering},
archivePrefix = "arXiv",
   eprint = {1302.3462},
     year = {2013},
     note = {preprint arXiv:1302.3642},
     ooktitle = {Proc Int Conf Audio and Acoustic Signal Processing (ICASSP)},
     booktitle = {Proceedings of the International Conference on Audio and Acoustic Signal Processing (ICASSP)},
     abstract= {Segregating an audio mixture containing multiple simultaneous bird sounds is a challenging task. However, birdsong often contains rapid pitch modulations, and these modulations carry information which may be of use in automatic recognition. In this paper we demonstrate that an improved spectrogram representation, based on the distribution derivative method, leads to improved performance of a segregation algorithm which uses a Markov renewal process model to track vocalisation patterns consisting of singing and silences. }
}

@article{Stowell:2013,
	Author = {Stowell, D. and Plumbley, M. D.},
	Title = {Segregating event streams and noise with a {M}arkov renewal process model},
	Year = {2013},
	Journal = {Journal of Machine Learning Research},
	volume = {14},
	pages = {1891--1916},
	Note = {preprint arXiv:1211.2972},
	url = {http://jmlr.org/papers/v14/stowell13a.html},
	abstract={We describe an inference task in which a set of timestamped event observations must be clustered into an unknown number of temporal sequences with independent and varying rates of observations. Various existing approaches to multi-object tracking assume a fixed number of sources and/or a fixed observation rate; we develop an approach to inferring structure in timestamped data produced by a mixture of an unknown and varying number of similar Markov renewal processes, plus independent clutter noise. The inference simultaneously distinguishes signal from noise as well as clustering signal observations into separate source streams. We illustrate the technique via synthetic experiments as well as an experiment to track a mixture of singing birds. Source code is available.
},
}

@inproceedings{Stowell:2012c,
	Author = {Stowell, D. and Plumbley, M. D.},
	Booktitle = {Proceedings of the European Signal Processing Conference (EUSIPCO)},
	Title = {Framewise heterodyne chirp analysis of birdsong},
	Pages = {2694--2698},
	Url = {http://c4dm.eecs.qmul.ac.uk/papers/2012/StowellPlumbley2012eusipco.pdf},
	Abstract = {Harmonic birdsong is often highly nonstationary, which suggests that standard FFT representations may be of limited suitability. Wavelet and chirplet techniques exist in the literature, but are not often applied to signals such as bird vocalisations, perhaps due to analysis complexity. In this paper we develop a single-scale chirp analysis (computationally accelerated using FFT) which can be treated as an ordinary time-series. We then study a sinusoidal representation simply derived from the peak bins of this time-series. We show that it can lead to improved species classification from birdsong.
},
	Year = {2012}}

@incollection{Stowell:2013b,
	Author = {Stowell, D. and Chew, E.},
	Booktitle = {Proceedings of the 9th International Symposium on Computer Music Modeling and Retrieval (CMMR)},
	Pages = {387--399},
	Keywords = {arcsml},
	series = {Lecture Notes in Computer Science},
	number = {7900},
	Title = {Maximum a Posteriori Estimation of Piecewise Arcs in Tempo Time-Series},
	abstract = {In musical performances with expressive tempo modulation, the tempo variation can be modelled as a sequence of tempo arcs. Previous authors have used this idea to estimate series of piecewise arc segments from data. In this paper we describe a probabilistic model for a time-series process of this nature, and use this to perform inference of single- and multi-level arc processes from data. We describe an efficient Viterbi-like process for MAP inference of arcs. Our approach is score-agnostic, and together with efficient inference allows for online analysis of performances including improvisations, and can predict immediate future tempo trajectories.
},
	Year = {2013},
}

@inproceedings{Stowell:2012b,
	Author = {Stowell, D. and Chew, E.},
	Booktitle = {Proceedings of the 9th International Symposium on Computer Music Modeling and Retrieval (CMMR)},
	Pages = {634--644},
	Keywords = {arcsml},
	Title = {Bayesian MAP estimation of piecewise arcs in tempo time-series},
	Url = {http://c4dm.eecs.qmul.ac.uk/papers/2012/StowellChew2012cmmr.pdf},
	abstract = {In musical performances with expressive tempo modulation, the tempo variation can be modelled as a sequence of tempo arcs. Previous authors have used this idea to estimate series of piecewise arc segments from data. In this paper we describe a probabilistic model for a time-series process of this nature, and use this to perform inference of single- and multi-level arc processes from data. We describe an efficient Viterbi-like process for MAP inference of arcs. Our approach is score-agnostic, and together with efficient inference allows for online analysis of performances including improvisations, and can predict immediate future tempo trajectories.
},
	Year = {2012},
}

@inproceedings{Stowell:2012a,
	Author = {Stowell, D. and Plumbley, M. D.},
	Booktitle = {Proceedings of the 5th International Workshop on Machine Learning and Music (MML12)},
	Title = {Multi-target pitch tracking of vibrato sources in noise using the {GM-PHD} filter},
	Url = {http://c4dm.eecs.qmul.ac.uk/papers/2012/StowellPlumbley2012mml.pdf},
	Year = {2012},
	Pages = {27--28},
	Abstract = {Probabilistic approaches to tracking often use single-source Bayesian models; applying these to multi-source tasks is problematic. We apply a principled multi-object tracking implementation, the Gaussian mixture probability hypothesis density filter, to track multiple sources having fixed pitch plus vibrato. We demonstrate high-quality filtering in a synthetic experiment, and find improved tracking using a richer feature set which captures underlying dynamics. Our implementation is available as open-source Python code.
},
}

@inproceedings{Stowell:2011f,
	Author = {Stowell, D. and Barthet, M. and Dixon, S. and Sandler, M.},
	Booktitle = {Proceedings of the Digital Economy All Hands Conference 2011},
	Title = {Musicology for the masses: Situating new audio technologies for musicology and music education},
	Url = {http://de2011.computing.dundee.ac.uk/wp-content/uploads/2011/10/Musicology-for-the-masses-Situating-new-audio-technologies-for-musicology-and-music-education.pdf},
	Year = {2011},
}

@inproceedings{Stowell:2011c,
	Author = {Stowell, D. and McLean, A.},
	Booktitle = {Proceedings of the {BCS HCI} 2011 Workshop - When Words Fail: What can Music Interaction tell us about {HCI}?},
	Month = {Jul},
	Title = {Live music-making: a rich open task requires a rich open interface},
	Year = {2011},
	Url = {http://mcl.open.ac.uk/Workshop/uploads/stowell_richopen.pdf},
	Abstract = {In this position paper we present some themes of our research, strands of which reflect our title's assertion in various ways. Our focus here is on live music-making, in par ticular improvised or part-improvised performances which incorporate digital technologies.
},
}

@article{Stowell:2012x,
	Author = {Stowell, D. and Dixon, S.},
	Journal = {British Journal of Music Education},
	Title = {Integration of informal music technologies in secondary school music lessons},
	Year = {2014},
	volume={13},
	issue={1},
	pages={19--39},
  doi={10.1017/S026505171300020X},
  url={http://c4dm.eecs.qmul.ac.uk/papers/2013/StowellDixon2013.pdf},
  abstract={Technologies such as YouTube, mobile phones and MP3 players are increasingly integrated into secondary school music in the UK. At the same time, the gap between formal and informal music learning is being bridged by the incorporation of students’ preferred music into class activities. We conducted an ethnographic study in two secondary schools in London, investigating the roles of technology in the negotiation of musical concepts in music classes. From this, we report some observations on the relation between formal/informal and authorised/unauthorised activities in class, and some specific observations on the role of YouTube, mobile phones and MP3 players in the class context. In the lessons we observed, these technologies functioned as part of a richly multimodal ecosystem of technologies, combining aspects of formal and informal use. This carries implications for how we plan for the use of technology in the delivery of music education.},
	}

@inproceedings{Stowell:2011d,
	Author = {Stowell, D. and Dixon, S.},
	Booktitle = {Proceedings of the 12th International Society for Music Information Retrieval (ISMIR) Conference},
	Title = {{MIR} in school? {L}essons from ethnographic observation of secondary school music classes},
	Pages = {347--352},
	Year = {2011}}

@incollection{Stowell:2012,
	Address = {London},
	Author = {Stowell, D. and McLean, A.},
	Booktitle = {Music and Human-Computer Interaction},
	Editor = {Holland, S. and Wilkie, K. and Mulholland, P. and Seago, A.},
	Isbn = {978-1-4471-2989-9},
	Publisher = {Springer},
	Series = {Springer Series on Cultural Computing},
	Title = {Live music-making: a rich open task requires a rich open interface},
	Url = {http://www.springer.com/computer/hci/book/978-1-4471-2989-9},
	Year = {2013},
}

@inproceedings{Stowell:2011a,
	Author = {Stowell, D.},
	Booktitle = {Proceedings of the International Computer Music Conference (ICMC)},
	Title = {Scheduling and composing with {R}isset eternal accelerando rhythms},
	Year = {2011},
	abstract = {Jean-Claude Risset described an ``eternal accelerando'' illusion, related to Shepard tones, in which a rhythm can be constructed to give the perception of continuous acceleration. The effect can in principle be derived from any rhythmic template, producing patterns with aspects of fractal self-similarity. Any attempt to use it compositionally must address the difficult issue of scheduling events (notes, beats) and structural changes in a way that integrates with the multi-layered self-similar rhythmic structure. In this paper we develop an approach to scheduling rhythms, melodies and structural changes over a Risset accelerando (or decelerando) framework. We derive the mappings which allow note sequences and sample playback to be incorporated. We discuss some compositional choices available within this framework, and demonstrate them via audio examples.},
	url = {http://c4dm.eecs.qmul.ac.uk/papers/2011/Stowell2011icmc.pdf},
}

@techreport{Stowell:2010e,
	Author = {Stowell, D. and Plumbley, M. D.},
	Institution = {Centre for Digital Music, Queen Mary University of London},
	Keywords = {birdsong},
	Month = {Aug},
	Number = {C4DM-TR-09-12},
	Title = {Birdsong and {C4DM}: A survey of {UK} birdsong and machine recognition for music researchers},
	Url = {http://c4dm.eecs.qmul.ac.uk/papers/2010/Stowell2010-C4DM-TR-09-12-birdsong.pdf},
	Year = {2010},
}

@inproceedings{Stowell:2010d,
	Author = {Stowell, D. and Plumbley, M. D.},
	Booktitle = {Proceedings of the 2010 Workshop on Applications of Pattern Analysis (WAPA2010)},
	Title = {Cross-associating unlabelled timbre distributions to create expressive musical mappings},
	Pages = {28--35},
	Year = {2010},
	Abstract = {In timbre remapping applications such as concatenative synthesis, an audio signal is used as a template, and a mapping process derives control data for some audio synthesis algorithm such that it produces a new audio signal approximating the perceived trajectory of the original sound. Timbre is a multidimensional attribute with interactions between dimensions, and the control and synthesised signals typically represent sounds with different timbral ranges, so it is non-trivial to design a search process which makes best use of the timbral variety available in the synthesiser. We first discuss our preliminary work applying standard machine-learning techniques for this purpose (PCA, self-organising maps), and the reasons they were not satisfactory. We then describe a novel regression-tree technique which learns associations between unlabelled multidimensional timbre distributions.
}}

@article{Stowell:2010c,
	Author = {Stowell, D. and Plumbley, M. D.},
	Doi = {10.1080/09298215.2010.512979},
	Journal = {Journal of New Music Research},
	Month = {Sep},
	Number = {3},
	Pages = {203--213},
	Title = {Delayed decision-making in real-time beatbox percussion classification},
	Volume = {39},
	Year = {2010},
	Abstract = {Real-time classification applied to a vocal percussion signal holds potential as an interface for live musical control. In this article we propose a novel approach to resolving the tension between the needs for low-latency reaction and reliable classification, by deferring the final classification decision until after a response has been initiated. We introduce a new dataset of annotated human beatbox recordings, and use it to study the optimal delay for classification accuracy. We then investigate the effect of such delayed decision-making on the quality of the audio output via a MUSHRA-type listening test. Our results show that the effect depends on the output audio type: for popular dance/pop drum sounds the acceptable delay is on the order of 12--35 ms.
},
}

@phdthesis{Stowell:2010,
	Author = {Stowell, D.},
	School = {School of Electronic Engineering and Computer Science, Queen Mary University of London},
	Title = {Making music through real-time voice timbre analysis: machine learning and timbral control},
	Url = {http://www.mcld.co.uk/thesis/},
	Year = {2010},
}

@article{Stowell:2011b,
	Author = {Stowell, D. and Plumbley, M. D.},
	Doi = {10.1080/09298215.2011.596938},
	Issue = {4},
	Journal = {Journal of New Music Research},
	Pages = {325--336},
	Title = {Learning timbre analogies from unlabelled data by multivariate tree regression},
	Volume = {40},
	Year = {2011},
	Abstract = {Applications such as concatenative synthesis (audio mosaicing) and query-by-examplerequire the ability to search a database using a sound which is qualitatively different from the actual desired result -- for example when using vocal queries to retrieve nonvocal sound. Standard query techniques such as nearest neighbours do not account for this difference between source and target; they perform retrieval but do not learn to make timbral analogies. This paper addresses this issue by considering timbral query as a multivariate regression problem from one timbre distribution onto another. We develop a novel variant of multivariate tree regression: given only a set of unlabelled and unpaired samples from two distributions on the same space, the regression learns a cross-associative mapping which assumes general similarities in structure of the two distributions, yet can accommodate differences in shape at various scales. We demonstrate the technique with a synthetic example and with a concatenative synthesiser.
},
}

@inproceedings{Stowell:2010a,
	Author = {Stowell, D. and Plumbley, M. D.},
	Booktitle = {Proceedings of Sound and Music Computing},
	Pages = {45--50},
	Title = {Timbre remapping through a regression-tree technique},
	Year = {2010},
	Abstract = {We consider the task of inferring associations between two differently-distributed and unlabelled sets of timbre data. This arises in applications such as concatenative synthesis/ audio mosaicing in which one audio recording is used to control sound synthesis through concatenating fragments of an unrelated source recording. Timbre is a multidimensional attribute with interactions between dimensions, so it is non-trivial to design a search process which makes best use of the timbral variety available in the source recording. We must be able to map from control signals whose timbre features have different distributions from the source material, yet labelling large collections of timbral sounds is often impractical, so we seek an unsupervised technique which can infer relationships between distributions. We present a regression tree technique which learns associations between two unlabelled multidimensional distributions, and apply the technique to a simple timbral concatenative synthesis system. We demonstrate numerically that the mapping makes better use of the source material than a nearest-neighbour search.
}}

@article{Stowell:2009,
	Author = {Stowell, D. and Robertson, A. and Bryan-Kinns, N. and Plumbley, M. D.},
	Doi = {10.1016/j.ijhcs.2009.05.007},
	Journal = {International Journal of Human-Computer Studies},
	Month = {Nov},
	Number = {11},
	Pages = {960--975},
	Title = {Evaluation of live human-computer music-making: quantitative and qualitative approaches},
	Volume = {67},
	Year = {2009},
	Abstract = {Live music-making using interactive systems is not completely amenable to traditional HCI evaluation metrics such as task-completion rates. In this paper we discuss quantitative and qualitative approaches which provide opportunities to evaluate the music-making interaction, accounting for aspects which cannot be directly measured or expressed numerically, yet which may be important for participants. We present case studies in the application of a qualitative method based on Discourse Analysis, and a quantitative method based on the Turing Test. We compare and contrast these methods with each other, and with other evaluation approaches used in the literature, and discuss factors affecting which evaluation method(s) are appropriate in a given context.
},
}

@article{Stowell:2009d,
	Author = {Stowell, D. and Plumbley, M. D.},
	Doi = {10.1109/LSP.2009.2017346},
	Journal = {{IEEE} Signal Processing Letters},
	Keywords = {Entropy, estimation, multidimensional systems, multidimensional signal processing},
	Month = {Jun},
	Number = {6},
	Pages = {537--540},
	Title = {Fast multidimensional entropy estimation by k-d partitioning},
	Volume = {16},
	Year = {2009},
}

@incollection{Stowell:2011,
	Address = {Cambridge, MA},
	Author = {Stowell, D.},
	Booktitle = {The {S}uper{C}ollider Book},
	Chapter = {25},
	Editor = {Wilson, S. and Cottle, D. and Collins, N.},
	Publisher = {MIT Press},
	Title = {Writing Unit Generator Plugins},
	Url = {http://supercolliderbook.net/},
	Year = {2011},
}

@inproceedings{Stowell:2008b,
	Author = {Stowell, D. and Plumbley, M. D.},
	Booktitle = {Proc. 11th Conference on Digital Audio Effects (DAFx-08)},
	Keywords = {voice, vocal, timbre, robust, features, entropy, singing, speech, beatboxing},
	Month = {Sep},
	Pages = {325--332},
	Title = {Robustness and independence of voice timbre features under live performance acoustic degradations},
	Url = {http://www.acoustics.hut.fi/dafx08/papers/dafx08_58.pdf},
	Year = {2008},
}

@techreport{Stowell:2008a,
	Address = {London, UK},
	Author = {Stowell, D. and Plumbley, M. D.},
	Institution = {Dept of Electronic Engineering, Queen Mary University of London},
	Number = {C4DM-TR-08-01},
	Title = {Characteristics of the beatboxing vocal style},
	Url = {http://c4dm.eecs.qmul.ac.uk/papers/2008/Stowell08-beatboxvocalstyle-C4DM-TR-08-01.pdf},
	Year = {2008},
}

@inproceedings{Stowell:2008,
	Author = {Stowell, D. and Plumbley, M. D. and Bryan-Kinns, N.},
	Booktitle = {{N}ew {I}nterfaces for {M}usical {E}xpression},
	Keywords = {Evaluation, qualitative methods, discourse analysis, voice, timbre, beatboxing},
	Pages = {81--86},
	Title = {Discourse analysis evaluation method for expressive musical interfaces},
	Url = {http://c4dm.eecs.qmul.ac.uk/papers/2008/StowellPlumbley08-nime.pdf},
	Year = {2008},
	Abstract = {In the NIME field there is an acknowledged paucity of reliable evaluation. Structured evaluation methods do exist, derived from other areas of HCI, but they largely focus on how precisely users can reproduce musical units. To evaluate the expressive and creative affordances of an interface, we need to go beyond precision; but these aspects are difficult to operationalise, particularly with quantitative methods. However, rigorous qualitative methods do exist and can be used to investigate such topics. We present a methodology based around user studies involving Discourse Analysis of speech. We also present an example of the methodology in use: we evaluate a musical interface which utilises vocal timbre, with a user group of beatboxers.},
}

@inproceedings{Stowell:2007,
	Author = {Stowell, D. and Plumbley, M. D.},
	Booktitle = {Proceedings of the International Computer Music Conference (ICMC'07)},
	Month = {Aug},
	Pages = {312--319},
	Title = {Adaptive whitening for improved real-time audio onset detection},
	Url = {http://c4dm.eecs.qmul.ac.uk/papers/2007/StowellPlumbley07-icmc.pdf},
	Volume = {2},
	Year = {2007},
	Abstract = {We describe a new method for preprocessing STFT phase-vocoder frames for improved performance in real-time onset detection, which we term ``adaptive whitening''. The procedure involves normalising the magnitude of each bin according to a recent maximum value for that bin, with the aim of allowing each bin to achieve a similar dynamic range over time, which helps mitigate against the influence of spectral roll-off and strongly-varying dynamics. Adaptive whitening requires no training, is relatively lightweight to compute, and can run in real-time. Yet it can improve onset detector performance by more than ten percentage points (peak F-measure) in some cases, and improves the performance of most of the onset detectors tested.

We present results demonstrating that adaptive whitening can significantly improve the performance of various STFT-based onset detection functions, including functions based on the power, spectral flux, phase deviation, and complex deviation measures. Our results find the process to be especially beneficial for certain types of audio signal (e.g. complex mixtures such as pop music). 
},
}

@misc{Stowell:2006,
	Author = {Stowell, D.},
	Howpublished = {SuperCollider symposium, Birmingham},
	Month = {Jul},
	Title = {{G}enetic {A}lgorithms and Live Evolution},
	Url = {http://mcld.co.uk/supercollider/DanStowell-GA-SCsymposium_2006.rtf},
	Year = {2006},
}

